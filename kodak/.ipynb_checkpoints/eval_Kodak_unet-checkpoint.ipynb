{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47d9ae3b-b989-4162-8549-f0704d94f6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c0a6a75-8509-4783-94d6-4fa2735d341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the UNet architecture exactly as in your code\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a44b37ec-e908-4d8c-a871-8715ee3a4341",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        # Encoder\n",
    "        self.conv1 = DoubleConv(1, 32)\n",
    "        self.conv2 = DoubleConv(32, 64)\n",
    "        self.conv3 = DoubleConv(64, 128)\n",
    "        \n",
    "        # Decoder\n",
    "        self.up_conv2 = DoubleConv(192, 64)  # 128 + 64 channels\n",
    "        self.up_conv1 = DoubleConv(96, 32)   # 64 + 32 channels\n",
    "        \n",
    "        # Final convolution\n",
    "        self.final_conv = nn.Conv2d(32, 3, kernel_size=1)\n",
    "        \n",
    "        # Pooling and upsampling\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        conv1 = self.conv1(x)           # 32x32 -> 32x32\n",
    "        x = self.pool(conv1)            # 32x32 -> 16x16\n",
    "        \n",
    "        conv2 = self.conv2(x)           # 16x16 -> 16x16\n",
    "        x = self.pool(conv2)            # 16x16 -> 8x8\n",
    "        \n",
    "        # Bridge\n",
    "        x = self.conv3(x)               # 8x8 -> 8x8\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.up(x)                  # 8x8 -> 16x16\n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "        x = self.up_conv2(x)            # 16x16 -> 16x16\n",
    "        \n",
    "        x = self.up(x)                  # 16x16 -> 32x32\n",
    "        x = torch.cat([x, conv1], dim=1)\n",
    "        x = self.up_conv1(x)            # 32x32 -> 32x32\n",
    "        \n",
    "        # Final convolution\n",
    "        x = self.final_conv(x)          # 32x32 -> 32x32\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64ad77a0-a488-4767-a7a8-5eec26109ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_kodak_image(model, input_path, gt_path, device, patch_size=32, overlap=2):\n",
    "    \"\"\"\n",
    "    Process a single pre-Bayered Kodak image through the UNet model\n",
    "    Args:\n",
    "        model: UNet model\n",
    "        input_path: Path to the input Bayer image\n",
    "        gt_path: Path to the ground truth image\n",
    "        device: torch device\n",
    "        patch_size: Size of patches to process\n",
    "        overlap: Overlap between patches\n",
    "    \"\"\"\n",
    "    # Load pre-Bayered image\n",
    "    bayer = cv2.imread(input_path, cv2.IMREAD_UNCHANGED)  # Read raw Bayer data\n",
    "    bayer = bayer.astype(np.float32) / 255.0  # 16-bit input\n",
    "    h, w = bayer.shape\n",
    "    \n",
    "    # Create bilinear interpolation for comparison\n",
    "    bayer_uint8 = (bayer * 255).astype(np.uint8)\n",
    "    bilinear = cv2.cvtColor(bayer_uint8, cv2.COLOR_BAYER_BG2RGB_EA)\n",
    "    bilinear = bilinear.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Load ground truth\n",
    "    ground_truth = cv2.imread(gt_path)\n",
    "    ground_truth = cv2.cvtColor(ground_truth, cv2.COLOR_BGR2RGB)\n",
    "    ground_truth = ground_truth.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Prepare output arrays\n",
    "    output = np.zeros((h, w, 3), dtype=np.float32)\n",
    "    weights = np.zeros((h, w, 3), dtype=np.float32)\n",
    "    \n",
    "    stride = patch_size - overlap\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Calculate positions for patches\n",
    "        y_positions = list(range(0, h - patch_size + 1, stride))\n",
    "        x_positions = list(range(0, w - patch_size + 1, stride))\n",
    "        \n",
    "        # Add final positions if needed\n",
    "        if h - patch_size not in y_positions:\n",
    "            y_positions.append(h - patch_size)\n",
    "        if w - patch_size not in x_positions:\n",
    "            x_positions.append(w - patch_size)\n",
    "        \n",
    "        for y in y_positions:\n",
    "            for x in x_positions:\n",
    "                # Extract patch\n",
    "                patch = bayer[y:y+patch_size, x:x+patch_size]\n",
    "                patch_tensor = torch.from_numpy(patch).float().unsqueeze(0).unsqueeze(0)\n",
    "                patch_tensor = patch_tensor.to(device)\n",
    "                \n",
    "                # Process patch\n",
    "                output_patch = model(patch_tensor)\n",
    "                output_patch = output_patch.squeeze().cpu().numpy()\n",
    "                output_patch = output_patch.transpose(1, 2, 0)\n",
    "                \n",
    "                # Create weight mask (gaussian falloff)\n",
    "                weight_mask = np.ones((patch_size, patch_size, 1))\n",
    "                if overlap > 0:\n",
    "                    for i in range(overlap):\n",
    "                        weight = np.exp(-((i - overlap/2)**2) / (2*(overlap/4)**2))\n",
    "                        weight_mask[i, :] *= weight\n",
    "                        weight_mask[-(i + 1), :] *= weight\n",
    "                        weight_mask[:, i] *= weight\n",
    "                        weight_mask[:, -(i + 1)] *= weight\n",
    "                \n",
    "                # Add to output with weights\n",
    "                output[y:y+patch_size, x:x+patch_size] += output_patch * weight_mask\n",
    "                weights[y:y+patch_size, x:x+patch_size] += weight_mask\n",
    "    \n",
    "    # Average overlapping regions\n",
    "    output = np.divide(output, weights, where=weights != 0)\n",
    "    \n",
    "    return output, bilinear, ground_truth, bayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "404cfdb4-24b2-4625-9559-477e9129fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psnr(img1, img2, max_val=1.0):\n",
    "    \"\"\"Calculate PSNR between two images\"\"\"\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    return 20 * np.log10(max_val / np.sqrt(mse))\n",
    "\n",
    "def calculate_cpsnr(img1, img2):\n",
    "    \"\"\"Calculate Color PSNR (average of R,G,B channels) and individual channel PSNRs.\"\"\"\n",
    "    psnr_r = calculate_psnr(img1[:,:,0], img2[:,:,0])\n",
    "    psnr_g = calculate_psnr(img1[:,:,1], img2[:,:,1])\n",
    "    psnr_b = calculate_psnr(img1[:,:,2], img2[:,:,2])\n",
    "    return psnr_r, psnr_g, psnr_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40139df2-a89c-4600-9f8d-bf2be63b0028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_kodak_dataset(model, device, input_dir='../dataset/kodak/input', \n",
    "                         gt_dir='../dataset/kodak/groundtruth'):\n",
    "    \"\"\"Process entire Kodak dataset\"\"\"\n",
    "\n",
    "    # Calculate PSNR for all test images\n",
    "    r_psnr_values = []\n",
    "    g_psnr_values = []\n",
    "    b_psnr_values = []\n",
    "\n",
    "    # Making new dir to save results\n",
    "    os.makedirs('result_kodak_dmcnn', exist_ok=True)\n",
    "    \n",
    "    # Get all input files\n",
    "    image_files = [f for f in os.listdir(input_dir) if f.endswith('.png')]\n",
    "    image_files.sort()\n",
    "    \n",
    "    \n",
    "    for i, img_file in enumerate(image_files):\n",
    "        print(f\"\\nProcessing image {i+1}/{len(image_files)}: {img_file}\")\n",
    "        \n",
    "        # Construct full paths\n",
    "        input_path = os.path.join(input_dir, img_file)\n",
    "        gt_path = os.path.join(gt_dir, img_file)\n",
    "        \n",
    "        if not os.path.exists(gt_path):\n",
    "            print(f\"Warning: Ground truth not found for {img_file}\")\n",
    "            continue\n",
    "        \n",
    "        # Process image\n",
    "        output, bilinear, ground_truth, bayer = process_kodak_image(\n",
    "            model, input_path, gt_path, device)\n",
    "\n",
    "        # Convert to uint8 and save\n",
    "        output_img = (output * 255).astype(np.uint8)\n",
    "        output_img = cv2.cvtColor(output_img, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite(f'result_kodak_dmcnn/{img_file}', output_img)\n",
    "        \n",
    "        # Calculate PSNR for each channel\n",
    "        r_psnr, g_psnr, b_psnr = calculate_cpsnr(output_img, ground_truth)\n",
    "        r_psnr_values.append(r_psnr)\n",
    "        g_psnr_values.append(g_psnr)\n",
    "        b_psnr_values.append(b_psnr)\n",
    "        \n",
    "    # Calculate statistics\n",
    "    mean_r_psnr = np.mean(r_psnr_values)\n",
    "    mean_g_psnr = np.mean(g_psnr_values)\n",
    "    mean_b_psnr = np.mean(b_psnr_values)\n",
    "    mean_cpsnr = (mean_r_psnr + mean_g_psnr + mean_b_psnr) / 3.0\n",
    "    \n",
    "    std_r_psnr = np.std(r_psnr_values)\n",
    "    std_g_psnr = np.std(g_psnr_values)\n",
    "    std_b_psnr = np.std(b_psnr_values)\n",
    "    std_cpsnr = np.std([r_psnr_values, g_psnr_values, b_psnr_values])\n",
    "    \n",
    "    print(f\"\\nTest Set Results:\")\n",
    "    print(f\"R channel - Mean: {mean_r_psnr:.2f} dB, Std: {std_r_psnr:.2f} dB\")\n",
    "    print(f\"G channel - Mean: {mean_g_psnr:.2f} dB, Std: {std_g_psnr:.2f} dB\")\n",
    "    print(f\"B channel - Mean: {mean_b_psnr:.2f} dB, Std: {std_b_psnr:.2f} dB\")\n",
    "    print(f\"CPSNR     - Mean: {mean_cpsnr:.2f} dB, Std: {std_cpsnr:.2f} dB\")\n",
    "\n",
    "        \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27f503f1-26fa-4bfb-8822-3314b2af1e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "# Initialize model\n",
    "model = UNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a93545f-5d3c-4419-a361-10d643c189c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2300112/2471046204.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('../best_unet_model.pth')\n"
     ]
    }
   ],
   "source": [
    " # Load trained weights\n",
    "checkpoint = torch.load('../best_unet_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded model from epoch {checkpoint['epoch']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c19dcc3f-056b-43b1-aa3a-b007e491b847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image 1/24: kodim01.png\n",
      "\n",
      "Processing image 2/24: kodim02.png\n",
      "\n",
      "Processing image 3/24: kodim03.png\n",
      "\n",
      "Processing image 4/24: kodim04.png\n",
      "\n",
      "Processing image 5/24: kodim05.png\n",
      "\n",
      "Processing image 6/24: kodim06.png\n",
      "\n",
      "Processing image 7/24: kodim07.png\n",
      "\n",
      "Processing image 8/24: kodim08.png\n",
      "\n",
      "Processing image 9/24: kodim09.png\n",
      "\n",
      "Processing image 10/24: kodim10.png\n",
      "\n",
      "Processing image 11/24: kodim11.png\n",
      "\n",
      "Processing image 12/24: kodim12.png\n",
      "\n",
      "Processing image 13/24: kodim13.png\n",
      "\n",
      "Processing image 14/24: kodim14.png\n",
      "\n",
      "Processing image 15/24: kodim15.png\n",
      "\n",
      "Processing image 16/24: kodim16.png\n",
      "\n",
      "Processing image 17/24: kodim17.png\n",
      "\n",
      "Processing image 18/24: kodim18.png\n",
      "\n",
      "Processing image 19/24: kodim19.png\n",
      "\n",
      "Processing image 20/24: kodim20.png\n",
      "\n",
      "Processing image 21/24: kodim21.png\n",
      "\n",
      "Processing image 22/24: kodim22.png\n",
      "\n",
      "Processing image 23/24: kodim23.png\n",
      "\n",
      "Processing image 24/24: kodim24.png\n",
      "\n",
      "Test Set Results:\n",
      "R channel - Mean: -39.86 dB, Std: 2.68 dB\n",
      "G channel - Mean: -41.24 dB, Std: 2.02 dB\n",
      "B channel - Mean: -41.85 dB, Std: 1.42 dB\n",
      "CPSNR     - Mean: -40.98 dB, Std: 2.26 dB\n"
     ]
    }
   ],
   "source": [
    "# Process Kodak dataset\n",
    "process_kodak_dataset(model, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
